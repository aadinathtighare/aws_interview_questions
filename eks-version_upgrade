Q. i hv eks cluster i want to do version upgrade so can I create new node group with new version in same eks cluster ?
Q. if I drain and cordan old nodes in old node group then how to migrate workload ?
its automatic or manually ?
ЁЯСЙ Workload migration is AUTOMATIC, but you trigger it manually.

Cordon/Drain тЖТ manual action (by you)

Pod rescheduling тЖТ automatic (by Kubernetes)
-------------------------------------------------------------------
PDB is VERY important exactly in the node drain / upgrade situation you described. тАЬDuring voluntary disruptions (like drain, upgrade), how many pods are allowed to go down at a time.тАЭ

It protects your application from downtime.

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: app-pdb
  namespace: default
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: my-app
-----------------------------
тЭМ PDB mhanje тАЬpod old node var chaluch pahijeтАЭ asa nahi

тЬЕ PDB mhanje тАЬcluster madhe kamit kami 1 pod running pahijeтАЭ

Pod:

Old node var asu shakto

Kinva new node var asu shakto
================
PDB ensures that during node draining, Kubernetes evicts pods gradually so that the minimum required pods remain available, enabling zero-downtime upgrades.
===============================================
ЁЯФ╣ Commands to test PDB behavior
# kubectl get pdb -A
# kubectl describe pdb app-pdb
===================================================
what is the kind for cluster autoscaler ?
Cluster Autoscaler (CA) itself is typically installed as a Deployment object.
kind: Deployment
-------------------------------------------------------
Q. in eks during eks version upgrade i created new node group with new version so after workload migration is completed from old node group to new node group 
which node group I can delete ?
--> In an EKS version upgrade scenario where you have already created a new node group and migrated your workloads, you should delete the Old Node Group (the one running the previous Kubernetes version).
------------------------
Q. in my case what i did i created new node-group with new version & upgrade the old node group ?
--> In your specific situation, since you have both a new node group (at the new version) and an upgraded old node group (now also at the new version), you technically have a "Blue-Green" setup where both groups are now identical in version.

You should delete the New Node Group that you created as a temporary migration target.
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. in aws console eks there is update option for node group so what exactly happen in background when we click in update ?
--> AWS Console рдордзреВрди рдЬреЗрд╡реНрд╣рд╛ рддреВ EKS Node Group рдЪреНрдпрд╛ "Update" рдмрдЯрдгрд╡рд░ рдХреНрд▓рд┐рдХ рдХрд░рддреЛрд╕, рддреЗрд╡реНрд╣рд╛ рдмреЕрдХрдЧреНрд░рд╛рдЙрдВрдбрдордзреНрдпреЗ рдПрдХ рдЦреВрдк рдорд╣рддреНрддреНрд╡рд╛рдЪреА рдкреНрд░реЛрд╕реЗрд╕ рдЪрд╛рд▓рддреЗ рдЬрд┐рд▓рд╛ "Rolling Update" рдореНрд╣рдгрддрд╛рдд.

рдпрд╛рдЪрд╛ рдореБрдЦреНрдп рдЙрджреНрджреЗрд╢ рд╣рд╛рдЪ рдЕрд╕рддреЛ рдХреА рддреБрдордЪреЗ Application рдЪрд╛рд▓реВ рдЕрд╕рддрд╛рдирд╛ (Zero Downtime) рдирд╡реАрди Nodes ре▓рдб рд╡реНрд╣рд╛рд╡реЗрдд рдЖрдгрд┐ рдЬреБрдиреЗ Nodes рдХрд╛рдвреВрди рдЯрд╛рдХрд╛рд╡реЗрдд.
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Q. suppose i m using terraform for eks version upgrade so i need to upgrade first control plane then add ons then worker nodes , how it possible in terraform ?
--> рд╣реЛ, Terraform рдордзреНрдпреЗ EKS рдЕрдкрдЧреНрд░реЗрдб рдХрд░рддрд╛рдирд╛ рд╣реАрдЪ рдкрджреНрдзрдд (Control Plane -> Add-ons -> Worker Nodes) рдлреЙрд▓реЛ рдХрд░рдгреЗ рдЧрд░рдЬреЗрдЪреЗ рдЖрд╣реЗ.
Terraform рдордзреНрдпреЗ рдПрдХрд╛рдЪ рд╡реЗрд│реА рд╕рд░реНрд╡ рд╡реНрд╣рд░реНрдЬрдиреНрд╕ рдмрджрд▓рдгреНрдпрд╛рдкреЗрдХреНрд╖рд╛, рдПрдХрд╛рдорд╛рдЧреВрди рдПрдХ apply рдХрд░рдгреЗ рд╕рд░реНрд╡рд╛рдд рд╕реБрд░рдХреНрд╖рд┐рдд рдЕрд╕рддреЗ.
Step 1: Control Plane Upgrade рдЖрдзреА рдлрдХреНрдд aws_eks_cluster рд░рд┐рд╕реЛрд░реНрд╕рдордзреАрд▓ version рдЕрдкрдбреЗрдЯ рдХрд░рд╛ (рдЙрджрд╛. 1.30 рд╡рд░реВрди 1.31).
# terraform apply -target=aws_eks_cluster.example

Step 2: Add-ons Upgrade рдХрдВрдЯреНрд░реЛрд▓ рдкреНрд▓реЗрди рдЕрдкрдбреЗрдЯ рдЭрд╛рд▓реНрдпрд╛рд╡рд░, aws_eks_addon рд░рд┐рд╕реЛрд░реНрд╕рдордзреНрдпреЗ рдЬрд╛рдКрди addon_version рдЕрдкрдбреЗрдЯ рдХрд░рд╛. рдирд╡реАрди K8s рд╡реНрд╣рд░реНрдЬрдирд▓рд╛ рд╕рдкреЛрд░реНрдЯ рдХрд░рдгрд╛рд░реАрдЪ рд╡реНрд╣рд░реНрдЬрди рдирд┐рд╡рдбрд╛.
# terraform apply -target=aws_eks_addon.vpc_cni

Step 3: Worker Nodes Upgrade рд╢реЗрд╡рдЯреА aws_eks_node_group рдордзреАрд▓ version рдХрд┐рдВрд╡рд╛ ami_type рдЕрдкрдбреЗрдЯ рдХрд░рд╛.
# terraform apply -target=aws_eks_node_group.example

---------------------------------
реи. рдХреЛрдбрдордзреНрдпреЗ 'Depends On' рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░рдгреЗ
рдЬрд░ рддреБрдореНрд╣рд╛рд▓рд╛ рд╕рд░реНрд╡ рдХрд╛рд╣реА рдПрдХрд╛рдЪ terraform apply рдордзреНрдпреЗ рдХрд░рд╛рдпрдЪреЗ рдЕрд╕реЗрд▓, рддрд░ рддреБрдореНрд╣реА depends_on рд╣реЗ рдЖрд░реНрдЧреБрдореЗрдВрдЯ рд╡рд╛рдкрд░реВрди рдХреНрд░рдо рдард░рд╡реВ рд╢рдХрддрд╛. рдпрд╛рдореБрд│реЗ Terraform рд▓рд╛ рдХрд│рддреЗ рдХреА рдЖрдзреА рдХрд╛рдп рдкреВрд░реНрдг рдЭрд╛рд▓реЗ рдкрд╛рд╣рд┐рдЬреЗ.
# 1. EKS Cluster (Control Plane)
resource "aws_eks_cluster" "my_cluster" {
  name     = "production-cluster"
  version  = "1.31" # рдирд╡реАрди рд╡реНрд╣рд░реНрдЬрди
  role_arn = aws_iam_role.cluster.arn
  # ... рдЗрддрд░ рдХреЙрдиреНрдлрд┐рдЧ
}

# 2. EKS Add-ons (Depends on Cluster)
resource "aws_eks_addon" "vpc_cni" {
  cluster_name  = aws_eks_cluster.my_cluster.name
  addon_name    = "vpc-cni"
  addon_version = "v1.18.1-eksbuild.1" 

  # рдХреНрд▓рд╕реНрдЯрд░ рдЕрдкрдбреЗрдЯ рдЭрд╛рд▓реНрдпрд╛рд╢рд┐рд╡рд╛рдп рд╣реЗ рд╕реБрд░реВ рд╣реЛрдгрд╛рд░ рдирд╛рд╣реА
  depends_on = [aws_eks_cluster.my_cluster]
}

# 3. Node Group (Depends on Add-ons/Cluster)
resource "aws_eks_node_group" "worker_nodes" {
  cluster_name    = aws_eks_cluster.my_cluster.name
  node_group_name = "standard-workers"
  version         = "1.31" # рдХреНрд▓рд╕реНрдЯрд░ рд╡реНрд╣рд░реНрдЬрдирд╢реА рдореЕрдЪ рдХрд░рд╛

  # рд╣реЗ рд╢реЗрд╡рдЯреВрди рдЕрдкрдбреЗрдЯ рд╣реЛрдИрд▓
  depends_on = [aws_eks_cluster.my_cluster, aws_eks_addon.vpc_cni]
}
============================================================================
Sequential Apply: рдореЛрдареНрдпрд╛ рдкреНрд░реЛрдбрдХреНрд╢рди рдХреНрд▓рд╕реНрдЯрд░рдордзреНрдпреЗ depends_on рдкреЗрдХреНрд╖рд╛ рдореЕрдиреНрдпреБрдЕрд▓реА рдПрдХрд╛рдорд╛рдЧреВрди рдПрдХ -target рдлреНрд▓реЕрдЧ рд╡рд╛рдкрд░реВрди рдЕрдкреНрд▓рд╛рдп рдХрд░рдгреЗ рдЬрд╛рд╕реНрдд 'Safe' рдорд╛рдирд▓реЗ рдЬрд╛рддреЗ, рдХрд╛рд░рдг рдпрд╛рдореБрд│реЗ рддреБрдореНрд╣рд╛рд▓рд╛ рдкреНрд░рддреНрдпреЗрдХ рдЯрдкреНрдкреНрдпрд╛рд╡рд░ рдПрд░рд░ рдЪреЗрдХ рдХрд░рддрд╛ рдпреЗрддрд╛рдд.
==========================================================================================================================
in terraform.tfvars 
we are giving values like ami_id for node group
add ons versions
--------------------------------------------

















